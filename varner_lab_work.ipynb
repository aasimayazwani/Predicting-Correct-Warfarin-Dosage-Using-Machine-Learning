{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taj07murTe0e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3hLbTORUhwd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "#calculating waiting time in the ER after \n",
    "class data:\n",
    "    def loading_lab_data(self):\n",
    "        import pandas as pd \n",
    "        lab = pd.read_csv(\"labevents.csv\")\n",
    "\n",
    "        #sodium = lab[lab.ITEMID == 50983]\n",
    "        INR = lab[lab.ITEMID == 51237]\n",
    "        INR_data = INR[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        INR_data = self.group(INR_data)\n",
    "        INR_data.columns = ['SUBJECT_ID', 'INR']\n",
    "\n",
    "        bilrubin = lab[lab.ITEMID == 50885]    \n",
    "        bilrubin_data = bilrubin[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        bilrubin_data = self.group(bilrubin_data)\n",
    "        bilrubin_data.columns = ['SUBJECT_ID', 'bilrubin']\n",
    "\n",
    "        PO2 = lab[lab.ITEMID == 50821]\n",
    "        PO2_data = PO2[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        PO2_data = self.group(PO2_data)\n",
    "        PO2_data.columns = ['SUBJECT_ID', 'PO2']\n",
    "\n",
    "        Oxygen = lab[lab.ITEMID == 50816]\n",
    "        Oxygen_data = Oxygen[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        Oxygen_data = self.group(Oxygen_data)\n",
    "        Oxygen_data.columns = ['SUBJECT_ID', 'Oxygen']\n",
    "\n",
    "        urea_nitrogen = lab[lab.ITEMID == 51006]\n",
    "        urea_nitrogen_data = urea_nitrogen[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urea_nitrogen_data = self.group(urea_nitrogen_data)\n",
    "        urea_nitrogen_data.columns = ['SUBJECT_ID', 'urea_nitrogen']\n",
    "\n",
    "        wbc = lab[lab.ITEMID == 51301]\n",
    "        wbc_data = wbc[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        wbc_data = self.group(wbc_data)\n",
    "        wbc_data.columns = ['SUBJECT_ID', 'wbc']\n",
    "\n",
    "        wbc_count = lab[lab.ITEMID == 51300]\n",
    "        wbc_count_data= wbc_count[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        wbc_count_data = self.group(wbc_count_data)\n",
    "        wbc_count_data.columns = ['SUBJECT_ID', 'wbc_count']\n",
    "\n",
    "        bicarbonate = lab[lab.ITEMID == 50882]\n",
    "        bicarbonate_data = bicarbonate[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        bicarbonate_data = self.group(bicarbonate_data)\n",
    "        bicarbonate_data.columns = ['SUBJECT_ID', 'bicarbonate']\n",
    "\n",
    "        sodium_whole_blood = lab[lab.ITEMID == 50983]\n",
    "        sodium_data = sodium_whole_blood[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        sodium_data = self.group(sodium_data)\n",
    "        sodium_data.columns = [\"SUBJECT_ID\",\"sodium_data\"]\n",
    "\n",
    "        potassium = lab[lab.ITEMID == 50971]\n",
    "        potassium_data = potassium[[\"SUBJECT_ID\",\"VALUE\"]] \n",
    "        potassium_data = self.group(potassium_data)\n",
    "        potassium_data.columns = [\"SUBJECT_ID\",\"potassium_data\"]\n",
    "\n",
    "        potassium_whole_blood = lab[lab.ITEMID == 50822]\n",
    "        potassium_whole_blood_data = potassium_whole_blood[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        potassium_whole_blood_data = self.group(potassium_whole_blood_data)\n",
    "        potassium_whole_blood_data.columns = [\"SUBJECT_ID\",\"potassium_whole_blood_data\"]\n",
    "\n",
    "\n",
    "        return INR_data, bilrubin_data, PO2_data, Oxygen_data, urea_nitrogen_data, wbc_data, wbc_count_data, bicarbonate_data, sodium_data, potassium_data, potassium_whole_blood_data\n",
    "\n",
    "    def mean_imputation(self,data):\n",
    "        return data.apply(lambda data: data.fillna(data.mean()),axis=0)\n",
    "    \n",
    "    def median_imputation(self,data):\n",
    "        return data.apply(lambda data: data.fillna(data.median()),axis=0)\n",
    "\n",
    "    def mode_imputation(self,data):\n",
    "        return data.apply(lambda data: data.fillna(data.mode()),axis=0)\n",
    "    \n",
    "    def flag_imputation(self,data):\n",
    "        import numpy as np \n",
    "        \"\"\"\n",
    "        The flag variable has three unqiue string entries (nan, 'abnormal', 'delta') which needs \n",
    "        to be imputed. \n",
    "        INPUT: This function takes only the complete dataset which should a column name FLAG \n",
    "        OUTPUT: This function outputs the entire dataset in imputed form. \n",
    "        nan - 0 \n",
    "        \"abnormal\" = 1, \"delta\" = 2, Nan = 0 \n",
    "        \"\"\"\n",
    "        new = data.FLAG.replace([\"abnormal\",\"delta\",np.NaN],[1,2,0])\n",
    "        data.FLAG = new \n",
    "        return data\n",
    "\n",
    "    \n",
    "    def chartevents(self):\n",
    "        import pandas as pd \n",
    "        data = pd.read_csv(\"chartevents.csv\")\n",
    "        \n",
    "        art_blood_pressure = data[data.ITEMID == 220050]\n",
    "        art_blood_pressure_data = art_blood_pressure[[\"SUBJECT_ID\",\"VALUENUM\",\"CHARTTIME\"]]\n",
    "        #art_blood_pressure_data = self.group_chart(art_blood_pressure_data)\n",
    "        art_blood_pressure_data.columns = [\"SUBJECT_ID\",\"art_blood_pressure\",\"CHARTTIME\"]\n",
    "        \n",
    "        temperature = data[data.ITEMID == 676]\n",
    "        temperature_data = temperature[[\"SUBJECT_ID\",\"VALUENUM\",\"CHARTTIME\"]]\n",
    "        #temperature_data = self.group_chart(temperature_data)\n",
    "        temperature_data.columns = [\"SUBJECT_ID\",\"temperature\",\"CHARTTIME\"]\n",
    "        \n",
    "        heart_rate = data[data.ITEMID == 211]\n",
    "        heart_rate_data = heart_rate[[\"SUBJECT_ID\",\"VALUENUM\",\"CHARTTIME\"]]\n",
    "        #heart_rate_data = self.group_chart(heart_rate_data)\n",
    "        heart_rate_data.columns = [\"SUBJECT_ID\",\"heart_rate\",\"CHARTTIME\"]\n",
    "        \n",
    "        return art_blood_pressure_data, temperature_data, heart_rate_data\n",
    "        \n",
    "    def loading_output_events(self):\n",
    "        import pandas as pd \n",
    "        data = pd.read_csv(\"outputevents.csv\")\n",
    "\n",
    "        l_nephrostomy = data[data.ITEMID==226565]\n",
    "        l_nephrostomy_data = l_nephrostomy[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        l_nephrostomy_data.columns = [\"SUBJECT_ID\",\"l_nephrostomy\"]\n",
    "\n",
    "        straight_cath = data[data.ITEMID==226567]\n",
    "        straight_cath_data = straight_cath[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        straight_cath_data.columns = [\"SUBJECT_ID\",\"straight_cath\"]\n",
    "\n",
    "        r_uternal_stent = data[data.ITEMID==226557]\n",
    "        r_uternal_stent_data = r_uternal_stent[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        r_uternal_stent_data.columns = [\"SUBJECT_ID\",\"r_uternal_stent\"]\n",
    "\n",
    "        l_uternal_stent = data[data.ITEMID==226558]\n",
    "        l_uternal_stent_data = l_uternal_stent[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        l_uternal_stent_data.columns = [\"SUBJECT_ID\",\"l_uternal_stent\"]\n",
    "\n",
    "        gu_irrigant_volume_in = data[data.ITEMID == 227488]\n",
    "        gu_irrigant_volume_in_data = gu_irrigant_volume_in[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        gu_irrigant_volume_in_data.columns = [\"SUBJECT_ID\",\"gu_irrigant_volume_in\"]\n",
    "\n",
    "        urine_volume_out = data[data.ITEMID == 227489]\n",
    "        urine_volume_out_data = urine_volume_out[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_volume_out_data.columns = [\"SUBJECT_ID\",\"urine_volume_out\"]    \n",
    "\n",
    "        urine_out_foley =  data[data.ITEMID == 40055]\n",
    "        urine_out_foley_data = urine_out_foley[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_foley_data.columns = [\"SUBJECT_ID\",\"urine_out_foley\"]\n",
    "\n",
    "        urine = data[data.ITEMID == 43175]\n",
    "        urine_data = urine[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_data.columns = [\"SUBJECT_ID\",\"urine\"]\n",
    "\n",
    "        urine_out_void = data[data.ITEMID == 40069]\n",
    "        urine_out_void_data = urine_out_void[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_void_data.columns = [\"SUBJECT_ID\",\"urine_out_void\"]\n",
    "\n",
    "        urine_out_condom_path = data[data.ITEMID == 40094]\n",
    "        urine_out_condom_path_data = urine_out_condom_path[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_condom_path_data.columns = [\"SUBJECT_ID\",\"urine_out_condom_path\"]\n",
    "\n",
    "        urine_out_suprapubic = data[data.ITEMID == 40715]\n",
    "        urine_out_suprapubic_data =urine_out_suprapubic[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_suprapubic_data.columns = [\"SUBJECT_ID\",\"urine_out_suprapubic\"]\n",
    "\n",
    "        urine_out_illeocoduit =  data[data.ITEMID == 40473]\n",
    "        urine_out_illeocoduit_data = urine_out_illeocoduit[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_illeocoduit_data.columns = [\"SUBJECT_ID\",\"urine_out_illeocoduit\"]\n",
    "\n",
    "        urine_out_incontinent =  data[data.ITEMID == 40085]\n",
    "        urine_out_incontinent_data = urine_out_incontinent[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_incontinent_data.columns = [\"SUBJECT_ID\",\"urine_out_incontinent\"]\n",
    "\n",
    "        urine_out_rt_nephrostomy = data[data.ITEMID == 40057]\n",
    "        urine_out_rt_nephrostomy_data = urine_out_rt_nephrostomy[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_rt_nephrostomy_data.columns = [\"SUBJECT_ID\",\"urine_out_rt_nephrostomy\"]\n",
    "\n",
    "        urine_out_lt_nephrostomy = data[data.ITEMID == 40056]\n",
    "        urine_out_lt_nephrostomy_data = urine_out_lt_nephrostomy[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_lt_nephrostomy_data.columns = [\"SUBJECT_ID\",\"urine_out_lt_nephrostomy\"]\n",
    "\n",
    "        urine_out_other = data[data.ITEMID == 40405]\n",
    "        urine_out_other_data = urine_out_other[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_other_data.columns = [\"SUBJECT_ID\",\"urine_out_other\"]\n",
    "\n",
    "        urine_out_straight_cath = data[data.ITEMID == 40428]\n",
    "        urine_out_straight_cath_data = urine_out_straight_cath[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_straight_cath_data.columns = [\"SUBJECT_ID\",\"urine_out_straight_cath\"]\n",
    "\n",
    "        orine_out_incontinent =  data[data.ITEMID == 40086]\n",
    "        orine_out_incontinent_data = orine_out_incontinent[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        orine_out_incontinent_data.columns = [\"SUBJECT_ID\",\"orine_out_incontinent\"]\n",
    "\n",
    "        urine_out_ureteral_stent1 =  data[data.ITEMID == 40096]\n",
    "        urine_out_ureteral_stent1_data = urine_out_ureteral_stent1[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_ureteral_stent1_data.columns = [\"SUBJECT_ID\",\"urine_out_ureteral_stent1\"]\n",
    "\n",
    "        urine_out_ureteral_stent2 =  data[data.ITEMID == 40651]\n",
    "        urine_out_ureteral_stent2_data = urine_out_ureteral_stent2[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        urine_out_ureteral_stent2_data.columns = [\"SUBJECT_ID\",\"urine_out_ureteral_stent2\"]\n",
    "\n",
    "        foley = data[data.ITEMID == 226559]\n",
    "        foley_data = foley[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        foley_data.columns = [\"SUBJECT_ID\",\"foley\"]\n",
    "\n",
    "        void =  data[data.ITEMID == 226560]\n",
    "        void_data = void[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        void_data.columns = [\"SUBJECT_ID\",\"void\"]\n",
    "\n",
    "        condom_cath = data[data.ITEMID == 226561]\n",
    "        condom_cath_data = condom_cath[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        condom_cath_data.columns = [\"SUBJECT_ID\",\"condom_cath\"]\n",
    "\n",
    "        illeoconduit = data[data.ITEMID == 226584]\n",
    "        illeoconduit_data = illeoconduit[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        illeoconduit_data.columns = [\"SUBJECT_ID\",\"illeoconduit\"]\n",
    "\n",
    "        suprapubic = data[data.ITEMID == 226563]\n",
    "        suprapubic_data = suprapubic[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        suprapubic_data.columns = [\"SUBJECT_ID\",\"suprapubic\"]\n",
    "\n",
    "        r_nephrostomy =  data[data.ITEMID == 226564]\n",
    "        r_nephrostomy_data = r_nephrostomy[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "        r_nephrostomy_data.columns = [\"SUBJECT_ID\",\"r_nephrostomy\"]\n",
    "\n",
    "        return l_nephrostomy_data, straight_cath_data, r_uternal_stent_data, l_uternal_stent_data, gu_irrigant_volume_in_data, urine_volume_out_data, urine_out_foley_data, urine_data, urine_out_void_data, urine_out_condom_path_data, urine_out_suprapubic_data,urine_out_illeocoduit_data,urine_out_incontinent_data, urine_out_rt_nephrostomy_data, urine_out_lt_nephrostomy_data, urine_out_other_data, urine_out_straight_cath_data, orine_out_incontinent_data, urine_out_ureteral_stent1_data, urine_out_ureteral_stent2_data, foley_data, void_data, condom_cath_data, illeoconduit_data, suprapubic_data,r_nephrostomy_data\n",
    "\n",
    "  \n",
    "    def group(self,data):\n",
    "        import pandas as pd \n",
    "        data['VALUE']=pd.to_numeric(data['VALUE'], errors='coerce').fillna(0)\n",
    "        mean = data.groupby(by = [\"SUBJECT_ID\"]).agg({'VALUE': 'mean'})\n",
    "        mean = mean.reset_index()\n",
    "        return mean \n",
    "\n",
    "    def group_chart(self,data):\n",
    "        import pandas as pd \n",
    "        data['VALUENUM']=pd.to_numeric(data['VALUENUM'], errors='coerce').fillna(0)\n",
    "        mean = data.groupby(by = [\"SUBJECT_ID\"]).agg({'VALUENUM': 'mean'})\n",
    "        mean = mean.reset_index()\n",
    "        return mean \n",
    "\n",
    "    \n",
    "    def merge(self,part1,part2):\n",
    "        import pandas as pd \n",
    "        return pd.merge(part1,part2,on = [\"SUBJECT_ID\"],how = \"outer\")\n",
    "\n",
    "    def merging_lab_events(self):\n",
    "        a,b,c,d,e,f,g,h,i,j,k = self.loading_lab_data()\n",
    "        merge1 = self.merge(a,b)\n",
    "        merge2 = self.merge(merge1,c)\n",
    "        merge3 = self.merge(merge2,d)\n",
    "        merge4 = self.merge(merge3,e)\n",
    "        merge5 = self.merge(merge4,f)\n",
    "        merge6 = self.merge(merge5,g)\n",
    "        merge7 = self.merge(merge6,h)\n",
    "        merge8 = self.merge(merge7,i)\n",
    "        merge9 = self.merge(merge8,j)\n",
    "        merge10 = self.merge(merge9,k)\n",
    "        return merge10\n",
    "\n",
    "\n",
    "    def merging_output_events(self):\n",
    "        a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z = self.loading_output_events()\n",
    "        merge1 = self.merge(a,b)\n",
    "        merge2 = self.merge(merge1,c)\n",
    "        merge3 = self.merge(merge2,d)\n",
    "        merge4 = self.merge(merge3,e)\n",
    "        merge5 = self.merge(merge4,f)\n",
    "        merge6 = self.merge(merge5,g)\n",
    "        merge7 = self.merge(merge6,h)\n",
    "        merge8 = self.merge(merge7,i)\n",
    "        merge9 = self.merge(merge8,j)\n",
    "        merge10 = self.merge(merge9,k)\n",
    "        merge11 = self.merge(merge10,l)\n",
    "        merge12 = self.merge(merge11,m)\n",
    "        merge13 = self.merge(merge12,n)\n",
    "        merge14 = self.merge(merge13,o)\n",
    "        merge15 = self.merge(merge14,p)\n",
    "        merge16 = self.merge(merge15,q)\n",
    "        merge17 = self.merge(merge16,r)\n",
    "        merge18 = self.merge(merge17,s)\n",
    "        merge19 = self.merge(merge18,t)\n",
    "        merge20 = self.merge(merge19,u)\n",
    "        merge21 = self.merge(merge20,v)\n",
    "        merge22 = self.merge(merge21,w)\n",
    "        merge23 = self.merge(merge22,x)\n",
    "        merge24 = self.merge(merge23,y)\n",
    "        merge25 = self.merge(merge24,z)\n",
    "        return merge25\n",
    "    \n",
    "    def merging_chartevents(self):\n",
    "        a,b,c = self.chartevents()\n",
    "        merge1 = self.merge(a,b)\n",
    "        merge2 = self.merge(merge1,c)\n",
    "        return merge2 \n",
    "    \n",
    "    def fill_missing_values_with(self,data,item):\n",
    "        return data.fillna(item)\n",
    "    \n",
    "    def percentage_missing_values(self,data):\n",
    "        return data.isnull().sum()/len(data)*100\n",
    "    \n",
    "    def do_lstm_model(self,df,ts,look_back,epochs,type_ = None,train_fraction = 0.67):\n",
    "        import numpy\n",
    "        import matplotlib.pyplot as plt\n",
    "        from pandas import read_csv\n",
    "        import math\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "        from keras.layers import LSTM\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        # Convert an array of values into a dataset matrix\n",
    "        def create_dataset(self, dataset, look_back=1):\n",
    "            dataX, dataY = [], []\n",
    "            for i in range(len(dataset)-look_back-1):\n",
    "                a = dataset[i:(i+look_back), 0]\n",
    "                dataX.append(a)\n",
    "                dataY.append(dataset[i + look_back, 0])\n",
    "            return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "        # Fix random seed for reproducibility\n",
    "        numpy.random.seed(7)\n",
    "\n",
    "        # Get dataset\n",
    "        dataset = df[ts].values\n",
    "        dataset = dataset.astype('float32')\n",
    "\n",
    "        # Normalize the dataset\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "\n",
    "        # Split into train and test sets\n",
    "        train_size = int(len(dataset) * train_fraction)\n",
    "        test_size = int(len(dataset) - train_size)\n",
    "        train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "        # Reshape into X=t and Y=t+1\n",
    "        look_back = look_back\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "        # Reshape input to be [samples, time steps, features]\n",
    "        if type_ == 'regression with time steps':\n",
    "            trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "            testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "        elif type_ == 'stacked with memory between batches':\n",
    "            trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "            testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "        else:\n",
    "            trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "        # Create and fit the LSTM network\n",
    "        batch_size = 1\n",
    "        model = Sequential()\n",
    "\n",
    "        if type_ == 'regression with time steps':\n",
    "            model.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "        elif type_ == 'memory between batches':\n",
    "            model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "        elif type_ == 'stacked with memory between batches':\n",
    "            model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "            model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "        else:\n",
    "            model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
    "            for i in range(100):\n",
    "                model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "                model.reset_states()\n",
    "            else:\n",
    "                model.fit(trainX,trainY,epochs = epochs,batch_size = 1,verbose = 2)\n",
    "\n",
    "        # Make predictions\n",
    "        if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
    "            trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "            testPredict = model.predict(testX, batch_size=batch_size)\n",
    "        else:\n",
    "            trainPredict = model.predict(trainX)\n",
    "            testPredict = model.predict(testX)\n",
    "\n",
    "        # Invert predictions\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)\n",
    "        trainY = scaler.inverse_transform([trainY])\n",
    "        testPredict = scaler.inverse_transform(testPredict)\n",
    "        testY = scaler.inverse_transform([testY])\n",
    "\n",
    "        # Calculate root mean squared error\n",
    "        trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "        print('Train Score: %.2f RMSE' % (trainScore))\n",
    "        testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "        print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "        # Shift train predictions for plotting\n",
    "        trainPredictPlot = numpy.empty_like(dataset)\n",
    "        trainPredictPlot[:, :] = numpy.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "        # Shift test predictions for plotting\n",
    "        testPredictPlot = numpy.empty_like(dataset)\n",
    "        testPredictPlot[:, :] = numpy.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "        # Plot baseline and predictions\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.plot(scaler.inverse_transform(dataset))\n",
    "        plt.plot(trainPredictPlot)\n",
    "        plt.plot(testPredictPlot)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def MICE_imputation(self,data):\n",
    "        col = data.columns.values \n",
    "        dt = data[\"CHARTTIME\"]\n",
    "        data.drop(columns = [\"CHARTTIME\"])\n",
    "        from fancyimpute import IterativeImputer as MICE\n",
    "        df1 = MICE().fit_transform(data)\n",
    "        df1 = pd.DataFrame(df1)\n",
    "        df1 = pd.concat([dt, df1], axis='col')\n",
    "        df1.columns = col\n",
    "        return df1\n",
    "\n",
    "\n",
    "        # ## 1. Requirements\n",
    "    def MIDA(self,b):\n",
    "        import os\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from math import sqrt\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.utils.data\n",
    "        import torch.optim as optim\n",
    "\n",
    "\n",
    "        # ## 2. Set Args\n",
    "        theta = 7\n",
    "        num_epochs = 5\n",
    "        dropout_ratio = 0.5\n",
    "        #data_path = 'data/BostonHousing.csv'\n",
    "        data_path = '~/Desktop/filled.csv'\n",
    "        mechanism = 'mcar'\n",
    "        method = 'uniform'\n",
    "        test_size = 0.3\n",
    "        use_cuda = False\n",
    "        batch_size  = 5 # not in the paper\n",
    "\n",
    "\n",
    "        # ## 3. Prepare Data\n",
    "        data = b.values\n",
    "        rows, cols = data.shape\n",
    "        shuffled_index = np.random.permutation(rows)\n",
    "        train_index = shuffled_index[:int(rows*(1-test_size))]\n",
    "        test_index = shuffled_index[int(rows*(1-test_size)):]\n",
    "        train_data = data[train_index, :]\n",
    "        test_data = data[test_index, :]\n",
    "        # standardized between 0 and 1\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(train_data)\n",
    "        train_data = scaler.transform(train_data)\n",
    "        test_data = scaler.transform(test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def missing_method(raw_data, mechanism='mcar', method='uniform') :\n",
    "            \n",
    "            data = raw_data.copy()\n",
    "            rows, cols = data.shape\n",
    "            # missingness threshold\n",
    "            t = 0.2\n",
    "            if mechanism == 'mcar' :\n",
    "                if method == 'uniform' :\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where v<=t\n",
    "                    mask = (v<=t)\n",
    "                    data[mask] = 0\n",
    "\n",
    "                elif method == 'random' :\n",
    "                    # only half of the attributes to have missing value\n",
    "                    missing_cols = np.random.choice(cols, cols//2)\n",
    "                    c = np.zeros(cols, dtype=bool)\n",
    "                    c[missing_cols] = True\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where v<=t\n",
    "                    mask = (v<=t)*c\n",
    "                    data[mask] = 0\n",
    "\n",
    "                else :\n",
    "                    print(\"Error : There are no such method\")\n",
    "                    raise\n",
    "            \n",
    "            elif mechanism == 'mnar' :\n",
    "                \n",
    "                if method == 'uniform' :\n",
    "                    # randomly sample two attributes\n",
    "                    sample_cols = np.random.choice(cols, 2)\n",
    "                    # calculate ther median m1, m2\n",
    "                    m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "                    m1 = data[:,sample_cols[0]] <= m1\n",
    "                    m2 = data[:,sample_cols[1]] >= m2\n",
    "                    m = (m1*m2)[:, np.newaxis]\n",
    "                    mask = m*(v<=t)\n",
    "                    data[mask] = 0\n",
    "\n",
    "\n",
    "                elif method == 'random' :\n",
    "                    # only half of the attributes to have missing value\n",
    "                    missing_cols = np.random.choice(cols, cols//2)\n",
    "                    c = np.zeros(cols, dtype=bool)\n",
    "                    c[missing_cols] = True\n",
    "                    # randomly sample two attributes\n",
    "                    sample_cols = np.random.choice(cols, 2)\n",
    "                    # calculate ther median m1, m2\n",
    "                    m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "                    m1 = data[:,sample_cols[0]] <= m1\n",
    "                    m2 = data[:,sample_cols[1]] >= m2\n",
    "                    m = (m1*m2)[:, np.newaxis]\n",
    "                    mask = m*(v<=t)*c\n",
    "                    data[mask] = 0\n",
    "\n",
    "                else :\n",
    "                    print(\"Error : There is no such method\")\n",
    "                    raise\n",
    "            \n",
    "            else :\n",
    "                print(\"Error : There is no such mechanism\")\n",
    "                raise\n",
    "                \n",
    "            return data, mask\n",
    "\n",
    "        missed_data, mask = missing_method(test_data, mechanism=mechanism, method=method)\n",
    "        missed_data = torch.from_numpy(missed_data).float()\n",
    "        train_data = torch.from_numpy(train_data).float()\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "        # ## 4. Define Model\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "        # In[7]:\n",
    "\n",
    "\n",
    "        class Autoencoder(nn.Module):\n",
    "            def __init__(self, dim):\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.dim = dim\n",
    "                self.drop_out = nn.Dropout(p=0.5)\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(dim+theta*0, dim+theta*1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*1, dim+theta*2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*2, dim+theta*3)\n",
    "                )\n",
    "                    \n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(dim+theta*3, dim+theta*2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*2, dim+theta*1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*1, dim+theta*0)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, self.dim)\n",
    "                x_missed = self.drop_out(x)\n",
    "                z = self.encoder(x_missed)\n",
    "                out = self.decoder(z)\n",
    "                out = out.view(-1, self.dim)\n",
    "                return out\n",
    "\n",
    "\n",
    "\n",
    "        model = Autoencoder(dim=cols).to(device)\n",
    "        # ##Define Loss and Optimizer\n",
    "        loss = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.01, nesterov=True)\n",
    "        # ## 6. Train Model\n",
    "        cost_list = []\n",
    "        early_stop = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            total_batch = len(train_data) // batch_size\n",
    "            \n",
    "            for i, batch_data in enumerate(train_loader):\n",
    "                batch_data = batch_data.to(device)\n",
    "                reconst_data = model(batch_data)\n",
    "                cost = loss(reconst_data, batch_data)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "                        \n",
    "                if (i+1) % (total_batch//2) == 0:\n",
    "                    print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                        %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "                    \n",
    "                # early stopping rule 1 : MSE < 1e-06\n",
    "                if cost.item() < 1e-06 :\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                    \n",
    "        #         early stopping rule 2 : simple moving average of length 5\n",
    "        #         sometimes it doesn't work well.\n",
    "        #         if len(cost_list) > 5 :\n",
    "        #            if cost.item() > np.mean(cost_list[-5:]):\n",
    "        #                early_stop = True\n",
    "        #                break\n",
    "                        \n",
    "                cost_list.append(cost.item())\n",
    "\n",
    "            if early_stop :\n",
    "                break\n",
    "                \n",
    "        print(\"Learning Finished!\")\n",
    "\n",
    "        # ## 7. Test Model\n",
    "        model.eval()\n",
    "        filled_data = model(missed_data.to(device))\n",
    "        filled_data = filled_data.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        rmse_sum = 0\n",
    "\n",
    "        for i in range(cols) :\n",
    "            if mask[:,i].sum() > 0 :\n",
    "                y_actual = test_data[:,i][mask[:,i]]\n",
    "                y_predicted = filled_data[:,i][mask[:,i]], sklearn\n",
    "                rmse = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "                rmse_sum += rmse\n",
    "\n",
    "        print(\"RMSE_SUM :\", rmse_sum)\n",
    "        filled_data = pd.DataFrame(filled_data)\n",
    "        #filled_data.to_csv(\"~/Desktop/MIDA_data.csv\")\n",
    "        return filled_data\n",
    "    \n",
    "    def method_random_forest(self,data,label):\n",
    "        \"\"\"\n",
    "        data : There needs to be two inputs dataframe and the label name \n",
    "        label: The label name needs to be having a string format.  \n",
    "        \n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd \n",
    "        import numpy as np\n",
    "        #Split the data into training and testing sets\n",
    "        Y = data[label]\n",
    "        X = data.loc[:, data.columns != label]\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(X, Y,test_size = 0.25, random_state = 42)\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        # Instantiate model with 1000 decision trees\n",
    "        rf = RandomForestRegressor(n_estimators = 50, random_state = 42)\n",
    "        # Train the model on training data\n",
    "        rf.fit(train_features, train_labels)\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = rf.predict(test_features)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - test_labels)\n",
    "        # Print out the mean absolute error (mae)\n",
    "        #print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "        #mape = 100 * (errors / test_labels)\n",
    "        # Calculate and display accuracy\n",
    "        #accuracy = 100 - np.mean(mape)\n",
    "        return np.sqrt(((predictions - test_labels ) ** 2).mean())\n",
    "        #return print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3YGbMgD9GEV"
   },
   "outputs": [],
   "source": [
    "c = data()\n",
    "merging = c.merging_output_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMl0wzMz9r9y"
   },
   "outputs": [],
   "source": [
    "merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R2M6Lbx9r6U"
   },
   "outputs": [],
   "source": [
    "res_from_others= GAN_code(lab_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLfasOsN_g5D"
   },
   "outputs": [],
   "source": [
    "res_from_others.to_csv(\"res_from_others.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqGYM4Xq9r3R"
   },
   "outputs": [],
   "source": [
    "col = lab_events.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHcW5l0C9rym"
   },
   "outputs": [],
   "source": [
    "res_from_others.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njYAz_Yz9rv4"
   },
   "outputs": [],
   "source": [
    "p = c.loading_output_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCj3A4eDUvrb"
   },
   "outputs": [],
   "source": [
    "def scaling(data):\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from sklearn.preprocessing import scale\n",
    "    return scale(data, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "\n",
    "def loading_data():\n",
    "    c = data()\n",
    "    stage1 = c.merging_lab_events()\n",
    "    stage2 = c.merging_chartevents()\n",
    "    new = pd.merge(stage1,stage2,how = \"outer\",on = [\"SUBJECT_ID\",\"CHARTTIME\"])\n",
    "    s = new.columns.values\n",
    "    t = scaling(new)\n",
    "    t = pd.DataFrame(t)\n",
    "    t.columns = s\n",
    "    return t \n",
    "\n",
    "def processed_data(data, types,error = False):\n",
    "    \"\"\"\n",
    "    types: mean, median, mode \n",
    "    calculate: if no it will return not return any error, if yes it will return the error based on random forest error.  \n",
    "    \"\"\"\n",
    "    new = data\n",
    "    col = new.columns.values \n",
    "    if types == \"mean\":\n",
    "        if error == False:\n",
    "            return c.mean_imputation(new)\n",
    "        else :\n",
    "            file = c.method_random_forest(c.mean_imputation(new),label = \"INR\")\n",
    "            return file\n",
    "\n",
    "    if types == \"median\":\n",
    "        if error == False:\n",
    "            return c.median_imputation(new)\n",
    "        else :\n",
    "            return c.method_random_forest(c.median_imputation(new),label= \"INR\")\n",
    "        \n",
    "    if types == \"mode\":\n",
    "        if error == False:\n",
    "            return c.mode_imputation(new) \n",
    "        else :\n",
    "            return c.method_random_forest(c.mode_imputation(new),label = \"INR\")\n",
    "\n",
    "    if types == \"mice\":\n",
    "        if error == False:\n",
    "            return c.mean_imputation(new)\n",
    "        else :\n",
    "            file = c.method_random_forest(c.MICE_imputation(new),label = \"INR\")\n",
    "            return file\n",
    "      \n",
    "    if types == \"mida\":\n",
    "        if error == False:\n",
    "          return c.mean_imputation(new)\n",
    "        else :\n",
    "          new = processed_data(new, types = \"mean\",error = False)\n",
    "          new = c.MIDA(processed_data(new, types = \"mean\",error = False))\n",
    "          new.columns = col \n",
    "          file = c.method_random_forest(data= new,label = \"INR\")\n",
    "          return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYBaWe9nJp_K"
   },
   "outputs": [],
   "source": [
    "def MIDA(b):\n",
    "        import os\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from math import sqrt\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.utils.data\n",
    "        import torch.optim as optim\n",
    "\n",
    "\n",
    "        # ## 2. Set Args\n",
    "        theta = 7\n",
    "        num_epochs = 5\n",
    "        dropout_ratio = 0.5\n",
    "        #data_path = 'data/BostonHousing.csv'\n",
    "        #data_path = '~/Desktop/filled.csv'\n",
    "        mechanism = 'mcar'\n",
    "        method = 'uniform'\n",
    "        test_size = 0.3\n",
    "        use_cuda = False\n",
    "        batch_size  = 5 # not in the paper\n",
    "\n",
    "\n",
    "        # ## 3. Prepare Data\n",
    "        data = b.values\n",
    "        rows, cols = data.shape\n",
    "        shuffled_index = np.random.permutation(rows)\n",
    "        train_index = shuffled_index[:int(rows*(1-test_size))]\n",
    "        test_index = shuffled_index[int(rows*(1-test_size)):]\n",
    "        train_data = data[train_index, :]\n",
    "        test_data = data[test_index, :]\n",
    "        # standardized between 0 and 1\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(train_data)\n",
    "        train_data = scaler.transform(train_data)\n",
    "        test_data = scaler.transform(test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def missing_method(raw_data, mechanism='mcar', method='uniform') :\n",
    "            \n",
    "            data = raw_data.copy()\n",
    "            rows, cols = data.shape\n",
    "            # missingness threshold\n",
    "            t = 0.2\n",
    "            if mechanism == 'mcar' :\n",
    "                if method == 'uniform' :\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where v<=t\n",
    "                    mask = (v<=t)\n",
    "                    data[mask] = 0\n",
    "\n",
    "                elif method == 'random' :\n",
    "                    # only half of the attributes to have missing value\n",
    "                    missing_cols = np.random.choice(cols, cols//2)\n",
    "                    c = np.zeros(cols, dtype=bool)\n",
    "                    c[missing_cols] = True\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where v<=t\n",
    "                    mask = (v<=t)*c\n",
    "                    data[mask] = 0\n",
    "\n",
    "                else :\n",
    "                    print(\"Error : There are no such method\")\n",
    "                    raise\n",
    "            \n",
    "            elif mechanism == 'mnar' :\n",
    "                \n",
    "                if method == 'uniform' :\n",
    "                    # randomly sample two attributes\n",
    "                    sample_cols = np.random.choice(cols, 2)\n",
    "                    # calculate ther median m1, m2\n",
    "                    m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "                    m1 = data[:,sample_cols[0]] <= m1\n",
    "                    m2 = data[:,sample_cols[1]] >= m2\n",
    "                    m = (m1*m2)[:, np.newaxis]\n",
    "                    mask = m*(v<=t)\n",
    "                    data[mask] = 0\n",
    "\n",
    "\n",
    "                elif method == 'random' :\n",
    "                    # only half of the attributes to have missing value\n",
    "                    missing_cols = np.random.choice(cols, cols//2)\n",
    "                    c = np.zeros(cols, dtype=bool)\n",
    "                    c[missing_cols] = True\n",
    "                    # randomly sample two attributes\n",
    "                    sample_cols = np.random.choice(cols, 2)\n",
    "                    # calculate ther median m1, m2\n",
    "                    m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "                    # uniform random vector\n",
    "                    v = np.random.uniform(size=(rows, cols))\n",
    "                    # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "                    m1 = data[:,sample_cols[0]] <= m1\n",
    "                    m2 = data[:,sample_cols[1]] >= m2\n",
    "                    m = (m1*m2)[:, np.newaxis]\n",
    "                    mask = m*(v<=t)*c\n",
    "                    data[mask] = 0\n",
    "\n",
    "                else :\n",
    "                    print(\"Error : There is no such method\")\n",
    "                    raise\n",
    "            \n",
    "            else :\n",
    "                print(\"Error : There is no such mechanism\")\n",
    "                raise\n",
    "                \n",
    "            return data, mask\n",
    "\n",
    "        missed_data, mask = missing_method(test_data, mechanism=mechanism, method=method)\n",
    "        missed_data = torch.from_numpy(missed_data).float()\n",
    "        train_data = torch.from_numpy(train_data).float()\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "        # ## 4. Define Model\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "        # In[7]:\n",
    "\n",
    "\n",
    "        class Autoencoder(nn.Module):\n",
    "            def __init__(self, dim):\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.dim = dim\n",
    "                self.drop_out = nn.Dropout(p=0.5)\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(dim+theta*0, dim+theta*1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*1, dim+theta*2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*2, dim+theta*3)\n",
    "                )\n",
    "                    \n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(dim+theta*3, dim+theta*2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*2, dim+theta*1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(dim+theta*1, dim+theta*0)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, self.dim)\n",
    "                x_missed = self.drop_out(x)\n",
    "                z = self.encoder(x_missed)\n",
    "                out = self.decoder(z)\n",
    "                out = out.view(-1, self.dim)\n",
    "                return out\n",
    "\n",
    "\n",
    "\n",
    "        model = Autoencoder(dim=cols).to(device)\n",
    "        # ##Define Loss and Optimizer\n",
    "        loss = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.01, nesterov=True)\n",
    "        # ## 6. Train Model\n",
    "        cost_list = []\n",
    "        early_stop = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            total_batch = len(train_data) // batch_size\n",
    "            \n",
    "            for i, batch_data in enumerate(train_loader):\n",
    "                batch_data = batch_data.to(device)\n",
    "                reconst_data = model(batch_data)\n",
    "                cost = loss(reconst_data, batch_data)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "                        \n",
    "                if (i+1) % (total_batch//2) == 0:\n",
    "                    print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                        %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "                    \n",
    "                # early stopping rule 1 : MSE < 1e-06\n",
    "                if cost.item() < 1e-06 :\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                    \n",
    "        #         early stopping rule 2 : simple moving average of length 5\n",
    "        #         sometimes it doesn't work well.\n",
    "        #         if len(cost_list) > 5 :\n",
    "        #            if cost.item() > np.mean(cost_list[-5:]):\n",
    "        #                early_stop = True\n",
    "        #                break\n",
    "                        \n",
    "                cost_list.append(cost.item())\n",
    "\n",
    "            if early_stop :\n",
    "                break\n",
    "                \n",
    "        print(\"Learning Finished!\")\n",
    "\n",
    "        # ## 7. Test Model\n",
    "        model.eval()\n",
    "        filled_data = model(missed_data.to(device))\n",
    "        filled_data = filled_data.cpu().detach().numpy()\n",
    "        return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36-isWmQVYhx"
   },
   "outputs": [],
   "source": [
    "c = MIDA(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMQ5YUyRtpN-"
   },
   "outputs": [],
   "source": [
    "c = pd.DataFrame(c)\n",
    "c.columns = data.columns.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXv9vHD61rtt"
   },
   "outputs": [],
   "source": [
    "c = c.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6kr9Ramj1r48",
    "outputId": "5dba9288-8343-4e20-c54c-a3ddad4f8f3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003278693606137761"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_random_forest(c,\"INR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BCGa453Wydw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "from pandas import get_dummies\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1lzudTZ5Xhr"
   },
   "outputs": [],
   "source": [
    "t.columns.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ma61RzpXXXw"
   },
   "outputs": [],
   "source": [
    "g=sns.pairplot(a, hue=\"INR\", size= 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POxiPQz6X6lx"
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\"\"\"merged_df = p \n",
    "merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "msno.matrix(merged_df[missingdata_df])\n",
    "msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "merged_df = pd.merge(train_df,properties_df)\n",
    "missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "msno.matrix(merged_df[missingdata_df])\"\"\"\n",
    "msno.heatmap(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm2rdDeBcnSW"
   },
   "outputs": [],
   "source": [
    "b = loading_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKIfPVaynAA6"
   },
   "outputs": [],
   "source": [
    "k = processed_data(data = b,types = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU8CPGq6olPX"
   },
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFifTBaPsG7G"
   },
   "outputs": [],
   "source": [
    "from missingpy import KNNImputer\n",
    "imputer = KNNImputer(missing_values=\"NaN\", n_neighbors=5, weights=\"uniform\", \n",
    "                 metric=\"masked_euclidean\", row_max_missing=0.5, \n",
    "                 col_max_missing=0.99, copy=True)\n",
    "X_imputed = imputer.fit_transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKLnieYHTiCt"
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "merged_df = b\n",
    "missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "msno.matrix(merged_df[missingdata_df])\n",
    "msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fsmx7cCiTjF3"
   },
   "outputs": [],
   "source": [
    "def VAE():\n",
    "  import random\n",
    "  import numpy as np\n",
    "  import tensorflow as tf\n",
    "\n",
    "  Normal = tf.contrib.distributions.Normal\n",
    "  np.random.seed(0)\n",
    "  tf.set_random_seed(0)\n",
    "\n",
    "  def xavier_init(fan_in, fan_out, constant=1): \n",
    "      \"\"\" Xavier initialization of network weights\"\"\"\n",
    "      # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "      low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "      high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "      return tf.random_uniform((fan_in, fan_out), \n",
    "                              minval=low, maxval=high, \n",
    "                              dtype=tf.float32)\n",
    "\n",
    "  class TFVariationalAutoencoder(object):\n",
    "      \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "      \n",
    "      This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "      distributions and realized by multi-layer perceptrons. The VAE can be learned\n",
    "      end-to-end.\n",
    "      \n",
    "      See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "      \"\"\"\n",
    "      def __init__(self, network_architecture, transfer_fct=tf.nn.relu, \n",
    "                  learning_rate=0.001, batch_size=100):\n",
    "          self.network_architecture = network_architecture\n",
    "          self.transfer_fct = transfer_fct\n",
    "          self.learning_rate = learning_rate\n",
    "          self.batch_size = batch_size\n",
    "          \n",
    "          # tf Graph input\n",
    "          self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "          \n",
    "          # Create autoencoder network\n",
    "          self._create_network()\n",
    "          \n",
    "          # Define loss function based variational upper-bound and \n",
    "          # corresponding optimizer\n",
    "          self._create_loss_optimizer()\n",
    "          \n",
    "          # Initializing the tensor flow variables\n",
    "          init = tf.global_variables_initializer()\n",
    "\n",
    "          # Launch the session\n",
    "          self.sess = tf.InteractiveSession()\n",
    "          self.sess.run(init)\n",
    "      \n",
    "      def _create_network(self):\n",
    "          # Initialize autoencode network weights and biases\n",
    "          network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "          # Use recognition network to determine mean and \n",
    "          # (log) variance of Gaussian distribution in latent\n",
    "          # space\n",
    "          self.z_mean, self.z_log_sigma_sq = \\\n",
    "              self._recognition_network(network_weights[\"weights_recog\"], \n",
    "                                        network_weights[\"biases_recog\"])\n",
    "\n",
    "          # Draw one sample z from Gaussian distribution\n",
    "          eps = tf.random_normal(tf.shape(self.z_mean), 0, 1, \n",
    "                                dtype=tf.float32)\n",
    "          # writing eps as above keeps self.z of the same size as the input, so\n",
    "          # it is not tied to a specific batch size as in the original (below)\n",
    "  #        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "  #                               dtype=tf.float32)\n",
    "          # z = mu + sigma*epsilon\n",
    "          self.z = tf.add(self.z_mean, \n",
    "                          tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "          # Use generator to determine mean and \n",
    "          # (log) variance of Gaussian distribution of reconstructed input\n",
    "          self.x_hat_mean, self.x_hat_log_sigma_sq = \\\n",
    "              self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                      network_weights[\"biases_gener\"])\n",
    "              \n",
    "      def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                              n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                              n_input, n_z):\n",
    "          all_weights = dict()\n",
    "          all_weights['weights_recog'] = {\n",
    "              'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "              'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "              'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "              'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "          all_weights['biases_recog'] = {\n",
    "              'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "              'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "              'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "              'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "          all_weights['weights_gener'] = {\n",
    "              'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "              'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "              'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "              'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "          all_weights['biases_gener'] = {\n",
    "              'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "              'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "              'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "              'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "          return all_weights\n",
    "              \n",
    "      def _recognition_network(self, weights, biases):\n",
    "          # Generate probabilistic encoder (recognition network), which\n",
    "          # maps inputs onto a normal distribution in latent space.\n",
    "          # The transformation is parametrized and can be learned.\n",
    "          layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                            biases['b1'])) \n",
    "          layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                            biases['b2'])) \n",
    "          z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                          biases['out_mean'])\n",
    "          z_log_sigma_sq = \\\n",
    "              tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                    biases['out_log_sigma'])\n",
    "          return (z_mean, z_log_sigma_sq)\n",
    "      \n",
    "      def _generator_network(self, weights, biases):\n",
    "          # Generate probabilistic decoder (decoder network), which\n",
    "          # maps points in latent space onto a normal distribution in data space.\n",
    "          # The transformation is parametrized and can be learned.\n",
    "          layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                            biases['b1'])) \n",
    "          layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                            biases['b2'])) \n",
    "          x_hat_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                          biases['out_mean'])\n",
    "          x_hat_log_sigma_sq = \\\n",
    "              tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                    biases['out_log_sigma'])\n",
    "          return (x_hat_mean, x_hat_log_sigma_sq)\n",
    "              \n",
    "      def _create_loss_optimizer(self):\n",
    "          # The loss is composed of two terms:\n",
    "          # 1.) The reconstruction loss (the negative log probability\n",
    "          #     of the input under the reconstructed Gaussian distribution \n",
    "          #     induced by the decoder in the data space).\n",
    "          #     This can be interpreted as the number of \"nats\" required\n",
    "          #     for reconstructing the input when the activation in latent\n",
    "          #     is given.\n",
    "          \n",
    "          X_hat_distribution = Normal(loc=self.x_hat_mean,\n",
    "                                      scale=tf.exp(self.x_hat_log_sigma_sq))\n",
    "          reconstr_loss = \\\n",
    "              -tf.reduce_sum(X_hat_distribution.log_prob(self.x), 1)\n",
    "              \n",
    "          # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "          ##    between the distribution in latent space induced by the encoder on \n",
    "          #     the data and some prior. This acts as a kind of regularizer.\n",
    "          #     This can be interpreted as the number of \"nats\" required\n",
    "          #     for transmitting the latent space distribution given\n",
    "          #     the prior.\n",
    "          latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                            - tf.square(self.z_mean) \n",
    "                                            - tf.exp(self.z_log_sigma_sq), 1)\n",
    "          self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "          \n",
    "  #        # Use ADAM optimizer\n",
    "  #        self.optimizer = \\\n",
    "  #            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "          \n",
    "          # Use RMSProp optimizer\n",
    "          self.optimizer = \\\n",
    "              tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "          \n",
    "      def partial_fit(self, X):\n",
    "          \"\"\"Train model based on mini-batch of input data.\n",
    "          \n",
    "          Return cost of mini-batch.\n",
    "          \"\"\"\n",
    "          opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                    feed_dict={self.x: X})\n",
    "          return cost\n",
    "      \n",
    "      def transform(self, X):\n",
    "          \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "          # Note: This maps to mean of distribution, we could alternatively\n",
    "          # sample from Gaussian distribution\n",
    "          return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "      \n",
    "      def generate(self, z_mu=None, n_samples = 100):\n",
    "          \"\"\" Generate data by sampling from latent space.\n",
    "          \n",
    "          If z_mu is not None, data for this point in latent space is\n",
    "          generated. Otherwise, z_mu is drawn from prior in latent \n",
    "          space.\n",
    "          \"\"\"\n",
    "          if z_mu is None:\n",
    "              z_mu = np.random.normal(size=[n_samples,self.network_architecture[\"n_z\"]])\n",
    "          \n",
    "          x_hat_mu, x_hat_logsigsq = self.sess.run((self.x_hat_mean, self.x_hat_log_sigma_sq), \n",
    "                              feed_dict={self.z: z_mu})\n",
    "          \n",
    "          eps = tf.random_normal(tf.shape(x_hat_mu), 0, 1, \n",
    "                                dtype=tf.float32)\n",
    "          \n",
    "          # x_hat_gen = mu + sigma*epsilon\n",
    "          x_hat_gen = tf.add(x_hat_mu, \n",
    "                          tf.multiply(tf.sqrt(tf.exp(x_hat_logsigsq)), eps))\n",
    "          \n",
    "          return x_hat_gen\n",
    "      \n",
    "      def reconstruct(self, X, sample = 'mean'):\n",
    "          \"\"\" Use VAE to reconstruct given data, using the mean of the \n",
    "              Gaussian distribution of the reconstructed variables by default, \n",
    "              as this gives better imputation results.\n",
    "              Data can also be reconstructed by sampling from the Gaussian\n",
    "              distribution of the reconstructed variables, by specifying the\n",
    "              input variable \"sample\" to value 'sample'.\n",
    "          \"\"\"\n",
    "          if sample == 'sample':\n",
    "              x_hat_mu, x_hat_logsigsq = self.sess.run((self.x_hat_mean, self.x_hat_log_sigma_sq), \n",
    "                              feed_dict={self.x: X})\n",
    "          \n",
    "              eps = tf.random_normal(tf.shape(X), 0, 1, \n",
    "                                dtype=tf.float32)\n",
    "              # x_hat = mu + sigma*epsilon\n",
    "              x_hat = tf.add(x_hat_mu, \n",
    "                          tf.multiply(tf.sqrt(tf.exp(x_hat_logsigsq)), eps))\n",
    "              # evaluate the tensor, as indexing into tensors seems to be a\n",
    "              # a missing function in tf:\n",
    "              x_hat = x_hat.eval()\n",
    "          else:\n",
    "              x_hat_mu = self.sess.run(self.x_hat_mean, \n",
    "                              feed_dict={self.x: X})\n",
    "              x_hat = x_hat_mu\n",
    "          \n",
    "          return x_hat\n",
    "      \n",
    "      def impute(self, X_corrupt, max_iter = 10):\n",
    "          \"\"\" Use VAE to impute missing values in X_corrupt. Missing values\n",
    "              are indicated by a NaN.\n",
    "          \"\"\"\n",
    "          # Select the rows of the datset which have one or more missing values:\n",
    "          NanRowIndex = np.where(np.isnan(np.sum(X_corrupt,axis=1)))\n",
    "          x_miss_val = X_corrupt[NanRowIndex[0],:]\n",
    "          \n",
    "          # initialise missing values with arbitrary value\n",
    "          NanIndex = np.where(np.isnan(x_miss_val))\n",
    "          x_miss_val[NanIndex] = 0\n",
    "          \n",
    "          MissVal = np.zeros([max_iter,len(NanIndex[0])], dtype=np.float32)\n",
    "          \n",
    "          for i in range(max_iter):\n",
    "              MissVal[i,:] = x_miss_val[NanIndex]\n",
    "              \n",
    "              # reconstruct the inputs, using the mean:\n",
    "              x_reconstruct = self.reconstruct(x_miss_val)\n",
    "              x_miss_val[NanIndex] = x_reconstruct[NanIndex]\n",
    "          \n",
    "          X_corrupt[NanRowIndex,:] = x_miss_val\n",
    "          X_imputed = X_corrupt\n",
    "          self.MissVal = MissVal\n",
    "          \n",
    "          return X_imputed\n",
    "      \n",
    "      def train(self, XData, training_epochs=10, display_step=10):\n",
    "          \"\"\" Train VAE in a loop, using numerical data\"\"\"\n",
    "          \n",
    "          def next_batch(Xdata,batch_size, MissingVals = False):\n",
    "              \"\"\" Randomly sample batch_size elements from the matrix of data, Xdata.\n",
    "                  Xdata is an [NxM] matrix, N observations of M variables.\n",
    "                  batch_size must be smaller than N.\n",
    "                  \n",
    "                  Returns Xdata_sample, a [batch_size x M] matrix.\n",
    "              \"\"\"\n",
    "              if MissingVals:\n",
    "                  # This returns records with any missing values replaced by 0:\n",
    "                  Xdata_length = Xdata.shape[0]\n",
    "                  X_indices = random.sample(range(Xdata_length),batch_size)\n",
    "                  Xdata_sample = np.copy(Xdata[X_indices,:])\n",
    "                  NanIndex = np.where(np.isnan(Xdata_sample))\n",
    "                  Xdata_sample[NanIndex] = 0\n",
    "              else:\n",
    "                  # This returns complete records only:\n",
    "                  ObsRowIndex = np.where(np.isfinite(np.sum(Xdata,axis=1)))\n",
    "                  X_indices = random.sample(list(ObsRowIndex[0]),batch_size)\n",
    "                  Xdata_sample = np.copy(Xdata[X_indices,:])\n",
    "              \n",
    "              return Xdata_sample\n",
    "          \n",
    "          # number of rows with complete entries in XData\n",
    "          NanRowIndex = np.where(np.isnan(np.sum(XData,axis=1)))\n",
    "          n_samples = np.size(XData, 0) - NanRowIndex[0].shape[0]\n",
    "          \n",
    "          losshistory = []\n",
    "          losshistory_epoch = []\n",
    "          for epoch in range(training_epochs):\n",
    "              avg_cost = 0\n",
    "              total_batch = int(n_samples / self.batch_size)\n",
    "              # Loop over all batches\n",
    "              for i in range(total_batch):\n",
    "                  batch_xs = next_batch(XData,self.batch_size, MissingVals = False)\n",
    "                  # Fit training using batch data\n",
    "                  cost = self.partial_fit(batch_xs)\n",
    "                  # Compute average loss\n",
    "                  avg_cost += cost / n_samples * self.batch_size\n",
    "              # Display logs per epoch step\n",
    "              if epoch % display_step == 0:\n",
    "                  losshistory_epoch.append(epoch)\n",
    "                  losshistory.append(-avg_cost)\n",
    "                  print(f'Epoch: {epoch+1:.4f} Cost= {avg_cost:.9f}')\n",
    "          self.losshistory = losshistory\n",
    "          self.losshistory_epoch = losshistory_epoch\n",
    "          return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjVktlPxilJU"
   },
   "outputs": [],
   "source": [
    "def GAN_code(Data):\n",
    "    \"\"\"\n",
    "    Data: Data which has missing values represented as np.nan values \n",
    "    Missing: Missing which is a mask matrix with missing values as 0 and non-missing values 1. \n",
    "    return type is a dataframe with all the values filled \n",
    "    \"\"\"\n",
    "    #%% Packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    #%% System Parameters\n",
    "    # 1. Mini batch size\n",
    "    mb_size = 128\n",
    "    # 2. Missing rate\n",
    "    p_miss = 0.2\n",
    "    # 3. Hint rate\n",
    "    p_hint = 0.9\n",
    "    # 4. Loss Hyperparameters\n",
    "    alpha = 10\n",
    "    # 5. Train Rate\n",
    "    train_rate = 0.8\n",
    "    # Data generation\n",
    "    def mask_matrix(df):\n",
    "        newdf = df.notnull().astype('int')\n",
    "        return newdf \n",
    "    #Data.drop(columns= ['Time'], inplace =True)\n",
    "    #Missing.drop(columns= ['Time'], inplace =True)\n",
    "    Missing = mask_matrix(Data)\n",
    "    Data = Data.fillna(0)\n",
    "    cols = Missing.columns.values\n",
    "    #cols = ['CW.FP.TONS', 'ELE.POWER','MASS.FLOW', 'OAT', 'ENTHALPY']\n",
    "    Missing = Missing[cols]\n",
    "    Data = np.array(Data)\n",
    "    Missing = np.array(Missing)\n",
    "    # Parameters\n",
    "    No = len(Data)\n",
    "    Dim = len(Data[0,:])\n",
    "    Train_No =  No\n",
    "    # Hidden state dimensions\n",
    "    H_Dim1 = Dim\n",
    "    H_Dim2 = Dim\n",
    "    H_Dim3 = Dim\n",
    "    # Normalization (0 to 1)\n",
    "    Min_Val = np.zeros(Dim)\n",
    "    Max_Val = np.zeros(Dim)\n",
    "    for i in range(Dim):\n",
    "        Min_Val[i] = np.min(Data[:,i])\n",
    "        Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "        Max_Val[i] = np.max(Data[:,i])\n",
    "        Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "    #%% Missing introducing\n",
    "    #p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "      \n",
    "    #Missing = np.zeros((No,Dim))\n",
    "\n",
    "    #for i in range(Dim):\n",
    "    #    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    #    B = A > p_miss_vec[i]\n",
    "    #    Missing[:,i] = 1.*B\n",
    "\n",
    "        \n",
    "    #%% Train Test Division    \n",
    "      \n",
    "    idx = np.random.permutation(No)\n",
    "\n",
    "    #Test_No = No - Train_No\n",
    "        \n",
    "    # Train / Test Features\n",
    "    trainX = Data\n",
    "    #testX = Data[idx[Train_No:],:]\n",
    "\n",
    "    # Train / Test Missing Indicators\n",
    "    trainM = Missing\n",
    "    #testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "    #%% Necessary Functions\n",
    "\n",
    "    # 1. Xavier Initialization Definition\n",
    "    def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "        return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "        \n",
    "    # Hint Vector Generation\n",
    "    def sample_M(m, n, p):\n",
    "        A = np.random.uniform(0., 1., size = [m, n])\n",
    "        B = A > p\n",
    "        C = 1.*B\n",
    "        return C\n",
    "      \n",
    "    '''\n",
    "    GAIN Consists of 3 Components\n",
    "    - Generator\n",
    "    - Discriminator\n",
    "    - Hint Mechanism\n",
    "    '''   \n",
    "      \n",
    "    #%% GAIN Architecture   \n",
    "      \n",
    "    #%% 1. Input Placeholders\n",
    "    # 1.1. Data Vector\n",
    "    X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.2. Mask Vector \n",
    "    M = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.3. Hint vector\n",
    "    H = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.4. X with missing values\n",
    "    New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "\n",
    "    #%% 2. Discriminator\n",
    "    D_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Hint as inputs\n",
    "    D_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "    D_b3 = tf.Variable(tf.zeros(shape = [Dim]))       # Output is multi-variate\n",
    "\n",
    "    theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "    #%% 3. Generator\n",
    "    G_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    G_W3 = tf.Variable(xavier_init([H_Dim2, H_Dim3]))\n",
    "    G_b3 = tf.Variable(tf.zeros(shape = [H_Dim3]))\n",
    "\n",
    "    G_W4 = tf.Variable(xavier_init([H_Dim3, Dim]))\n",
    "    G_b4 = tf.Variable(tf.zeros(shape = [Dim]))\n",
    "\n",
    "    theta_G = [G_W1, G_W2, G_W3, G_W4, G_b1, G_b2, G_b3, G_b4]\n",
    "\n",
    "    #%% GAIN Function\n",
    "\n",
    "    #%% 1. Generator\n",
    "    def generator(new_x,m):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n",
    "        G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "        G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "        G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3)+G_b2)\n",
    "        G_prob = tf.nn.sigmoid(tf.matmul(G_h3, G_W4) + G_b4) # [0,1] normalized Output\n",
    "        \n",
    "        return G_prob\n",
    "        \n",
    "    #%% 2. Discriminator\n",
    "    def discriminator(new_x, h):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n",
    "        D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "        D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "        D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "        D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "        \n",
    "        return D_prob\n",
    "\n",
    "    #%% 3. Other functions\n",
    "    # Random sample generator for Z\n",
    "    def sample_Z(m, n):\n",
    "        return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "    # Mini-batch generation\n",
    "    def sample_idx(m, n):\n",
    "        A = np.random.permutation(m)\n",
    "        idx = A[:n]\n",
    "        return idx\n",
    "\n",
    "        #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "    G_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "\n",
    "    D_loss = D_loss1\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    #MSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n",
    "\n",
    "    #%% Solver\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "    # Sessions\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #%% Iterations\n",
    "\n",
    "    #%% Start Iterations\n",
    "    for it in tqdm(range(2000)):    \n",
    "        \n",
    "        #%% Inputs\n",
    "        mb_idx = sample_idx(Train_No, mb_size)\n",
    "        X_mb = trainX[mb_idx,:]  \n",
    "        \n",
    "        Z_mb = sample_Z(mb_size, Dim) \n",
    "        M_mb = trainM[mb_idx,:]  \n",
    "        H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "        H_mb = M_mb * H_mb1\n",
    "        \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "        \n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "        _, G_loss_curr, MSE_train_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss],feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "                \n",
    "            \n",
    "        #%% Intermediate Losses\n",
    "        if it % 100 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n",
    "            #print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n",
    "            print()\n",
    "\n",
    "        \n",
    "    New_X = generator(Data,tf.to_float(Missing))\n",
    "    # Combine with original data\n",
    "    Hat_New_X = Data * Missing + New_X * (1-Missing)\n",
    "    final = pd.DataFrame(sess.run(Hat_New_X))\n",
    "    Data =  pd.DataFrame(Data)\n",
    "    Missing =  pd.DataFrame(Missing)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nrr3iZRmWcm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = k[['SUBJECT_ID', 'bilrubin', 'PO2', 'Oxygen', 'urea_nitrogen',\n",
    "       'wbc', 'wbc_count', 'bicarbonate', 'sodium_data', 'potassium_data',\n",
    "       'potassium_whole_blood_data', 'art_blood_pressure', 'temperature',\n",
    "       'heart_rate']]\n",
    "y = k['INR']\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "x_pca = pca.fit_transform(X_train)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "x_pca.columns=['PC1','PC2']\n",
    "plt.scatter(x_pca[\"PC1\"],x_pca[\"PC2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "_ItMwZJUBmiT",
    "outputId": "1de31c38-42e9-4a96-e941-18f7c43da5d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SUBJECT_ID', 'INR', 'bilrubin', 'PO2', 'Oxygen', 'urea_nitrogen',\n",
       "       'wbc', 'wbc_count', 'bicarbonate', 'sodium_data', 'potassium_data',\n",
       "       'potassium_whole_blood_data', 'art_blood_pressure', 'temperature',\n",
       "       'heart_rate'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.columns.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcX5nkcNPeVs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DCQvDHv071I"
   },
   "outputs": [],
   "source": [
    "kkt = c.merging_output_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6qYeiHOe1l1R",
    "outputId": "5a4da563-508d-4eba-c45e-64d90eb79f2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2115185, 13)"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8p3Tbx8cWEFA"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 1.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIT7cRn82J8V"
   },
   "outputs": [],
   "source": [
    "def EM_algorithm():\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import random as rand\n",
    "  import matplotlib.pyplot as plt\n",
    "  from scipy.stats import norm\n",
    "  from sys import maxint\n",
    "\n",
    "  ### Setup\n",
    "  # set random seed\n",
    "  rand.seed(42)\n",
    "\n",
    "  # 2 clusters\n",
    "  # not that both covariance matrices are diagonal\n",
    "  mu1 = [0, 5]\n",
    "  sig1 = [ [2, 0], [0, 3] ]\n",
    "\n",
    "  mu2 = [5, 0]\n",
    "  sig2 = [ [4, 0], [0, 1] ]\n",
    "\n",
    "  # generate samples\n",
    "  x1, y1 = np.random.multivariate_normal(mu1, sig1, 100).T\n",
    "  x2, y2 = np.random.multivariate_normal(mu2, sig2, 100).T\n",
    "\n",
    "  xs = np.concatenate((x1, x2))\n",
    "  ys = np.concatenate((y1, y2))\n",
    "  labels = ([1] * 100) + ([2] * 100)\n",
    "\n",
    "  data = {'x': xs, 'y': ys, 'label': labels}\n",
    "  df = pd.DataFrame(data=data)\n",
    "\n",
    "  # inspect the data\n",
    "  df.head()\n",
    "  df.tail()\n",
    "\n",
    "  fig = plt.figure()\n",
    "  plt.scatter(data['x'], data['y'], 24, c=data['label'])\n",
    "  fig.savefig(\"true-values.png\")\n",
    "\n",
    "  ### Expectation-maximization\n",
    "\n",
    "  # initial guesses - intentionally bad\n",
    "  guess = { 'mu1': [1,1],\n",
    "            'sig1': [ [1, 0], [0, 1] ],\n",
    "            'mu2': [4,4],\n",
    "            'sig2': [ [1, 0], [0, 1] ],\n",
    "            'lambda': [0.4, 0.6]\n",
    "          }\n",
    "\n",
    "  # probability that a point came from a Guassian with given parameters\n",
    "  # note that the covariance must be diagonal for this to work\n",
    "  def prob(val, mu, sig, lam):\n",
    "    p = lam\n",
    "    for i in range(len(val)):\n",
    "      p *= norm.pdf(val[i], mu[i], sig[i][i])\n",
    "    return p\n",
    "\n",
    "\n",
    "  # assign every data point to its most likely cluster\n",
    "  def expectation(dataFrame, parameters):\n",
    "    for i in range(dataFrame.shape[0]):\n",
    "      x = dataFrame['x'][i]\n",
    "      y = dataFrame['y'][i]\n",
    "      p_cluster1 = prob([x, y], list(parameters['mu1']), list(parameters['sig1']), parameters['lambda'][0] )\n",
    "      p_cluster2 = prob([x, y], list(parameters['mu2']), list(parameters['sig2']), parameters['lambda'][1] )\n",
    "      if p_cluster1 > p_cluster2:\n",
    "        dataFrame['label'][i] = 1\n",
    "      else:\n",
    "        dataFrame['label'][i] = 2\n",
    "    return dataFrame\n",
    "\n",
    "\n",
    "  # update estimates of lambda, mu and sigma\n",
    "  def maximization(dataFrame, parameters):\n",
    "    points_assigned_to_cluster1 = dataFrame[dataFrame['label'] == 1]\n",
    "    points_assigned_to_cluster2 = dataFrame[dataFrame['label'] == 2]\n",
    "    percent_assigned_to_cluster1 = len(points_assigned_to_cluster1) / float(len(dataFrame))\n",
    "    percent_assigned_to_cluster2 = 1 - percent_assigned_to_cluster1\n",
    "    parameters['lambda'] = [percent_assigned_to_cluster1, percent_assigned_to_cluster2 ]\n",
    "    parameters['mu1'] = [points_assigned_to_cluster1['x'].mean(), points_assigned_to_cluster1['y'].mean()]\n",
    "    parameters['mu2'] = [points_assigned_to_cluster2['x'].mean(), points_assigned_to_cluster2['y'].mean()]\n",
    "    parameters['sig1'] = [ [points_assigned_to_cluster1['x'].std(), 0 ], [ 0, points_assigned_to_cluster1['y'].std() ] ]\n",
    "    parameters['sig2'] = [ [points_assigned_to_cluster2['x'].std(), 0 ], [ 0, points_assigned_to_cluster2['y'].std() ] ]\n",
    "    return parameters\n",
    "\n",
    "  # get the distance between points\n",
    "  # used for determining if params have converged\n",
    "  def distance(old_params, new_params):\n",
    "    dist = 0\n",
    "    for param in ['mu1', 'mu2']:\n",
    "      for i in range(len(old_params)):\n",
    "        dist += (old_params[param][i] - new_params[param][i]) ** 2\n",
    "    return dist ** 0.5\n",
    "\n",
    "  # loop until parameters converge\n",
    "  shift = maxint\n",
    "  epsilon = 0.01\n",
    "  iters = 0\n",
    "  df_copy = df.copy()\n",
    "  # randomly assign points to their initial clusters\n",
    "  df_copy['label'] = map(lambda x: x+1, np.random.choice(2, len(df)))\n",
    "  params = pd.DataFrame(guess)\n",
    "\n",
    "  while shift > epsilon:\n",
    "    iters += 1\n",
    "    # E-step\n",
    "    updated_labels = expectation(df_copy.copy(), params)\n",
    "\n",
    "    # M-step\n",
    "    updated_parameters = maximization(updated_labels, params.copy())\n",
    "\n",
    "    # see if our estimates of mu have changed\n",
    "    # could incorporate all params, or overall log-likelihood\n",
    "    shift = distance(params, updated_parameters)\n",
    "\n",
    "    # logging\n",
    "    print(\"iteration {}, shift {}\".format(iters, shift))\n",
    "\n",
    "    # update labels and params for the next iteration\n",
    "    df_copy = updated_labels\n",
    "    params = updated_parameters\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(df_copy['x'], df_copy['y'], 24, c=df_copy['label'])\n",
    "    fig.savefig(\"iteration{}.png\".format(iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju8fPAFkC5T9"
   },
   "outputs": [],
   "source": [
    "EM_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwvxqZviC64q"
   },
   "outputs": [],
   "source": [
    "import fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNmdmDVHDRrM"
   },
   "outputs": [],
   "source": [
    "X_incomplete = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_tydWvAErkt"
   },
   "outputs": [],
   "source": [
    "new =pd.to_numeric(complete, errors='coerce').fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgJVcNV_J3jt"
   },
   "outputs": [],
   "source": [
    "complete.INR.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCZYDOroKAJs"
   },
   "outputs": [],
   "source": [
    "k = pd.to_numeric(complete[\"INR\"], errors='coerce').fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-buvLy9L6_w"
   },
   "outputs": [],
   "source": [
    "df = complete\n",
    "df[cols] = df[cols].applymap(lambda x: np.nan if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WwktQhQYMl8f",
    "outputId": "78ea1236-2361-4f0e-aacf-f27076aee385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.INR.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTUaHqRdMqIP"
   },
   "outputs": [],
   "source": [
    "from keras.re import l1l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYt5x5Z4NPhB"
   },
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/47001413/how-to-replace-any-strings-with-nan-in-pandas-dataframe-using-a-boolean-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpe_u_vuTlYN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Generate corrupted MNIST images by adding noise with normal dist\n",
    "# centered at 0.5 and std=0.5def GAN_code(Data):\n",
    "    \"\"\"\n",
    "    Data: Data which has missing values represented as np.nan values \n",
    "    Missing: Missing which is a mask matrix with missing values as 0 and non-missing values 1. \n",
    "    return type is a dataframe with all the values filled \n",
    "    \"\"\"\n",
    "    #%% Packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    #%% System Parameters\n",
    "    # 1. Mini batch size\n",
    "    mb_size = 128\n",
    "    # 2. Missing rate\n",
    "    p_miss = 0.2\n",
    "    # 3. Hint rate\n",
    "    p_hint = 0.9\n",
    "    # 4. Loss Hyperparameters\n",
    "    alpha = 10\n",
    "    # 5. Train Rate\n",
    "    train_rate = 0.8\n",
    "    # Data generation\n",
    "    def mask_matrix(df):\n",
    "        newdf = df.notnull().astype('int')\n",
    "        return newdf \n",
    "    #Data.drop(columns= ['Time'], inplace =True)\n",
    "    #Missing.drop(columns= ['Time'], inplace =True)\n",
    "    Missing = mask_matrix(Data)\n",
    "    Data = Data.fillna(0)\n",
    "    cols = Missing.columns.values\n",
    "    #cols = ['CW.FP.TONS', 'ELE.POWER','MASS.FLOW', 'OAT', 'ENTHALPY']\n",
    "    Missing = Missing[cols]\n",
    "    Data = np.array(Data)\n",
    "    Missing = np.array(Missing)\n",
    "    # Parameters\n",
    "    No = len(Data)\n",
    "    Dim = len(Data[0,:])\n",
    "    Train_No =  No\n",
    "    # Hidden state dimensions\n",
    "    H_Dim1 = Dim\n",
    "    H_Dim2 = Dim\n",
    "    H_Dim3 = Dim\n",
    "    # Normalization (0 to 1)\n",
    "    \"\"\"Min_Val = np.zeros(Dim)\n",
    "                Max_Val = np.zeros(Dim)\n",
    "                for i in range(Dim):\n",
    "                    Min_Val[i] = np.min(Data[:,i])\n",
    "                    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "                    Max_Val[i] = np.max(Data[:,i])\n",
    "                    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)   \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    scaler = StandardScaler().fit(Data) \n",
    "\n",
    "    #%% Missing introducing\n",
    "    #p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "      \n",
    "    #Missing = np.zeros((No,Dim))\n",
    "\n",
    "    #for i in range(Dim):\n",
    "    #    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    #    B = A > p_miss_vec[i]\n",
    "    #    Missing[:,i] = 1.*B\n",
    "\n",
    "        \n",
    "    #%% Train Test Division    \n",
    "      \n",
    "    idx = np.random.permutation(No)\n",
    "\n",
    "    #Test_No = No - Train_No\n",
    "        \n",
    "    # Train / Test Features\n",
    "    trainX = Data\n",
    "    #testX = Data[idx[Train_No:],:]\n",
    "\n",
    "    # Train / Test Missing Indicators\n",
    "    trainM = Missing\n",
    "    #testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "    #%% Necessary Functions\n",
    "\n",
    "    # 1. Xavier Initialization Definition\n",
    "    def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "        return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "        \n",
    "    # Hint Vector Generation\n",
    "    def sample_M(m, n, p):\n",
    "        A = np.random.uniform(0., 1., size = [m, n])\n",
    "        B = A > p\n",
    "        C = 1.*B\n",
    "        return C\n",
    "      \n",
    "    '''\n",
    "    GAIN Consists of 3 Components\n",
    "    - Generator\n",
    "    - Discriminator\n",
    "    - Hint Mechanism\n",
    "    '''   \n",
    "      \n",
    "    #%% GAIN Architecture   \n",
    "      \n",
    "    #%% 1. Input Placeholders\n",
    "    # 1.1. Data Vector\n",
    "    X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.2. Mask Vector \n",
    "    M = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.3. Hint vector\n",
    "    H = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.4. X with missing values\n",
    "    New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "\n",
    "    #%% 2. Discriminator\n",
    "    D_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Hint as inputs\n",
    "    D_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "    D_b3 = tf.Variable(tf.zeros(shape = [Dim]))       # Output is multi-variate\n",
    "\n",
    "    theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "    #%% 3. Generator\n",
    "    G_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    G_W3 = tf.Variable(xavier_init([H_Dim2, H_Dim3]))\n",
    "    G_b3 = tf.Variable(tf.zeros(shape = [H_Dim3]))\n",
    "\n",
    "    G_W4 = tf.Variable(xavier_init([H_Dim3, Dim]))\n",
    "    G_b4 = tf.Variable(tf.zeros(shape = [Dim]))\n",
    "\n",
    "    theta_G = [G_W1, G_W2, G_W3, G_W4, G_b1, G_b2, G_b3, G_b4]\n",
    "\n",
    "    #%% GAIN Function\n",
    "\n",
    "    #%% 1. Generator\n",
    "    def generator(new_x,m):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n",
    "        G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "        G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "        G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3)+G_b2)\n",
    "        G_prob = tf.nn.sigmoid(tf.matmul(G_h3, G_W4) + G_b4) # [0,1] normalized Output\n",
    "        \n",
    "        return G_prob\n",
    "        \n",
    "    #%% 2. Discriminator\n",
    "    def discriminator(new_x, h):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n",
    "        D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "        D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "        D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "        D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "        return D_prob\n",
    "\n",
    "    #%% 3. Other functions\n",
    "    # Random sample generator for Z\n",
    "    def sample_Z(m, n):\n",
    "        return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "    # Mini-batch generation\n",
    "    def sample_idx(m, n):\n",
    "        A = np.random.permutation(m)\n",
    "        idx = A[:n]\n",
    "        return idx\n",
    "\n",
    "        #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "    G_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "\n",
    "    D_loss = D_loss1\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    #MSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n",
    "\n",
    "    #%% Solver\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "    # Sessions\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #%% Iterations\n",
    "\n",
    "    #%% Start Iterations\n",
    "    for it in tqdm(range(2000)):    \n",
    "        \n",
    "        #%% Inputs\n",
    "        mb_idx = sample_idx(Train_No, mb_size)\n",
    "        X_mb = trainX[mb_idx,:]  \n",
    "        \n",
    "        Z_mb = sample_Z(mb_size, Dim) \n",
    "        M_mb = trainM[mb_idx,:]  \n",
    "        H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "        H_mb = M_mb * H_mb1\n",
    "        \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "        \n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "        _, G_loss_curr, MSE_train_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss],\n",
    "                                                                          feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "                \n",
    "            \n",
    "        #%% Intermediate Losses\n",
    "        if it % 100 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n",
    "            #print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n",
    "            print()\n",
    "\n",
    "    \n",
    "    New_X = generator(Data,tf.to_float(Missing))\n",
    "    # Combine with original data\n",
    "    Hat_New_X = Data * Missing + New_X * (1-Missing)\n",
    "    final = pd.DataFrame(sess.run(Hat_New_X))\n",
    "    Data =  pd.DataFrame(Data)\n",
    "    col = Data.columns.values \n",
    "    Data  = pd.DataFrame(scaler.transform(Data), columns = col)\n",
    "    Missing =  pd.DataFrame(Missing)\n",
    "    return Data\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
    "x_train_noisy = x_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
    "x_test_noisy = x_test + noise\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Encoder/Decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32, 64]\n",
    "\n",
    "# Build the Autoencoder Model\n",
    "# First build the Encoder Model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "# Instantiate Encoder Model\n",
    "encoder = Model(inputs, latent, name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Build the Decoder Model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train_noisy,\n",
    "                x_train,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                epochs=30,\n",
    "                batch_size=batch_size)\n",
    "\n",
    "# Predict the Autoencoder output from corrupted test images\n",
    "x_decoded = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "# Display the 1st 8 corrupted and denoised images\n",
    "rows, cols = 10, 30\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Input:  third rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I2nPeWZYA3F"
   },
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE7ZMEWUwHd6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWwJem6lpxp-"
   },
   "outputs": [],
   "source": [
    "from fancyimpute import SimpleFill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTH4wCGfp6Si"
   },
   "outputs": [],
   "source": [
    "X_filled_knn = KNN(k=3).fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WWEm4zb2Sly"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"outputevents.csv\")\n",
    "\n",
    "l_nephrostomy = data[data.ITEMID==226565]\n",
    "l_nephrostomy_data = l_nephrostomy[[\"SUBJECT_ID\",\"VALUE\"]]\n",
    "l_nephrostomy_data.columns = [\"SUBJECT_ID\",\"l_nephrostomy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tUeqV3KeCO8M",
    "outputId": "65d01e9e-702d-4ec2-cf6e-05e0e7dcda9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/pro-data\n"
     ]
    }
   ],
   "source": [
    "cd pro-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdP4pwhpjKCR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"coagulation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfJErtjgjN40"
   },
   "outputs": [],
   "source": [
    "def GAN_code(Data):\n",
    "    \"\"\"\n",
    "    Data: Data which has missing values represented as np.nan values \n",
    "    Missing: Missing which is a mask matrix with missing values as 0 and non-missing values 1. \n",
    "    return type is a dataframe with all the values filled \n",
    "    \"\"\"\n",
    "    #%% Packages\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    #%% System Parameters\n",
    "    # 1. Mini batch size\n",
    "    mb_size = 128\n",
    "    # 2. Missing rate\n",
    "    p_miss = 0.2\n",
    "    # 3. Hint rate\n",
    "    p_hint = 0.9\n",
    "    # 4. Loss Hyperparameters\n",
    "    alpha = 10\n",
    "    # 5. Train Rate\n",
    "    train_rate = 0.8\n",
    "    # Data generation\n",
    "    def mask_matrix(df):\n",
    "        newdf = df.notnull().astype('int')\n",
    "        return newdf \n",
    "    #Data.drop(columns= ['Time'], inplace =True)\n",
    "    #Missing.drop(columns= ['Time'], inplace =True)\n",
    "    Missing = mask_matrix(Data)\n",
    "    Data = Data.fillna(0)\n",
    "    cols = Missing.columns.values\n",
    "    #cols = ['CW.FP.TONS', 'ELE.POWER','MASS.FLOW', 'OAT', 'ENTHALPY']\n",
    "    Missing = Missing[cols]\n",
    "    Data = np.array(Data)\n",
    "    Missing = np.array(Missing)\n",
    "    # Parameters\n",
    "    No = len(Data)\n",
    "    Dim = len(Data[0,:])\n",
    "    Train_No =  No\n",
    "    # Hidden state dimensions\n",
    "    H_Dim1 = Dim\n",
    "    H_Dim2 = Dim\n",
    "    H_Dim3 = Dim\n",
    "    # Normalization (0 to 1)\n",
    "    \"\"\"Min_Val = np.zeros(Dim)\n",
    "                Max_Val = np.zeros(Dim)\n",
    "                for i in range(Dim):\n",
    "                    Min_Val[i] = np.min(Data[:,i])\n",
    "                    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "                    Max_Val[i] = np.max(Data[:,i])\n",
    "                    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)   \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    scaler = StandardScaler().fit(Data) \n",
    "\n",
    "    #%% Missing introducing\n",
    "    #p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "      \n",
    "    #Missing = np.zeros((No,Dim))\n",
    "\n",
    "    #for i in range(Dim):\n",
    "    #    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    #    B = A > p_miss_vec[i]\n",
    "    #    Missing[:,i] = 1.*B\n",
    "\n",
    "        \n",
    "    #%% Train Test Division    \n",
    "      \n",
    "    idx = np.random.permutation(No)\n",
    "\n",
    "    #Test_No = No - Train_No\n",
    "        \n",
    "    # Train / Test Features\n",
    "    trainX = Data\n",
    "    #testX = Data[idx[Train_No:],:]\n",
    "\n",
    "    # Train / Test Missing Indicators\n",
    "    trainM = Missing\n",
    "    #testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "    #%% Necessary Functions\n",
    "\n",
    "    # 1. Xavier Initialization Definition\n",
    "    def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "        return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "        \n",
    "    # Hint Vector Generation\n",
    "    def sample_M(m, n, p):\n",
    "        A = np.random.uniform(0., 1., size = [m, n])\n",
    "        B = A > p\n",
    "        C = 1.*B\n",
    "        return C\n",
    "      \n",
    "    '''\n",
    "    GAIN Consists of 3 Components\n",
    "    - Generator\n",
    "    - Discriminator\n",
    "    - Hint Mechanism\n",
    "    '''   \n",
    "      \n",
    "    #%% GAIN Architecture   \n",
    "      \n",
    "    #%% 1. Input Placeholders\n",
    "    # 1.1. Data Vector\n",
    "    X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.2. Mask Vector \n",
    "    M = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.3. Hint vector\n",
    "    H = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "    # 1.4. X with missing values\n",
    "    New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "\n",
    "    #%% 2. Discriminator\n",
    "    D_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Hint as inputs\n",
    "    D_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "    D_b3 = tf.Variable(tf.zeros(shape = [Dim]))       # Output is multi-variate\n",
    "\n",
    "    theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "    #%% 3. Generator\n",
    "    G_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "    G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "    G_W3 = tf.Variable(xavier_init([H_Dim2, H_Dim3]))\n",
    "    G_b3 = tf.Variable(tf.zeros(shape = [H_Dim3]))\n",
    "\n",
    "    G_W4 = tf.Variable(xavier_init([H_Dim3, Dim]))\n",
    "    G_b4 = tf.Variable(tf.zeros(shape = [Dim]))\n",
    "\n",
    "    theta_G = [G_W1, G_W2, G_W3, G_W4, G_b1, G_b2, G_b3, G_b4]\n",
    "\n",
    "    #%% GAIN Function\n",
    "\n",
    "    #%% 1. Generator\n",
    "    def generator(new_x,m):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n",
    "        G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "        G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "        G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3)+G_b2)\n",
    "        G_prob = tf.nn.sigmoid(tf.matmul(G_h3, G_W4) + G_b4) # [0,1] normalized Output\n",
    "        \n",
    "        return G_prob\n",
    "        \n",
    "    #%% 2. Discriminator\n",
    "    def discriminator(new_x, h):\n",
    "        inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n",
    "        D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "        D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "        D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "        D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "        return D_prob\n",
    "\n",
    "    #%% 3. Other functions\n",
    "    # Random sample generator for Z\n",
    "    def sample_Z(m, n):\n",
    "        return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "    # Mini-batch generation\n",
    "    def sample_idx(m, n):\n",
    "        A = np.random.permutation(m)\n",
    "        idx = A[:n]\n",
    "        return idx\n",
    "\n",
    "        #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "    G_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "\n",
    "    D_loss = D_loss1\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    #MSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n",
    "\n",
    "    #%% Solver\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "    # Sessions\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #%% Iterations\n",
    "\n",
    "    #%% Start Iterations\n",
    "    for it in tqdm(range(2000)):    \n",
    "        \n",
    "        #%% Inputs\n",
    "        mb_idx = sample_idx(Train_No, mb_size)\n",
    "        X_mb = trainX[mb_idx,:]  \n",
    "        \n",
    "        Z_mb = sample_Z(mb_size, Dim) \n",
    "        M_mb = trainM[mb_idx,:]  \n",
    "        H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "        H_mb = M_mb * H_mb1\n",
    "        \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "        \n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "        _, G_loss_curr, MSE_train_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss],\n",
    "                                                                          feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "                \n",
    "            \n",
    "        #%% Intermediate Losses\n",
    "        if it % 100 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n",
    "            #print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n",
    "            print()\n",
    "\n",
    "    \n",
    "    New_X = generator(Data,tf.to_float(Missing))\n",
    "    # Combine with original data\n",
    "    Hat_New_X = Data * Missing + New_X * (1-Missing)\n",
    "    final = pd.DataFrame(sess.run(Hat_New_X))\n",
    "    Data =  pd.DataFrame(Data)\n",
    "    col = Data.columns.values \n",
    "    Data  = pd.DataFrame(scaler.transform(Data), columns = col)\n",
    "    Missing =  pd.DataFrame(Missing)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a6euEaUT_qRD",
    "outputId": "bda0497f-bc89-480a-f003-ba5c7fb74000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2000 [00:00<07:51,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Train_loss: 8.382e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 141/2000 [00:00<01:21, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n",
      "Train_loss: 8.506e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 241/2000 [00:01<00:23, 73.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "Train_loss: 8.152e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 341/2000 [00:01<00:10, 156.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 300\n",
      "Train_loss: 8.326e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 441/2000 [00:02<00:07, 215.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 400\n",
      "Train_loss: 8.828e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 541/2000 [00:02<00:06, 233.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 500\n",
      "Train_loss: 8.652e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 641/2000 [00:03<00:05, 240.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 600\n",
      "Train_loss: 8.629e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 740/2000 [00:03<00:05, 239.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 700\n",
      "Train_loss: 8.712e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 837/2000 [00:03<00:04, 238.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 800\n",
      "Train_loss: 8.707e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 933/2000 [00:04<00:04, 236.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 900\n",
      "Train_loss: 8.75e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 1033/2000 [00:04<00:04, 236.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1000\n",
      "Train_loss: 8.149e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 1130/2000 [00:05<00:03, 238.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1100\n",
      "Train_loss: 8.962e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|   | 1229/2000 [00:05<00:03, 242.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1200\n",
      "Train_loss: 8.866e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 1329/2000 [00:05<00:02, 243.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1300\n",
      "Train_loss: 8.418e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|  | 1429/2000 [00:06<00:02, 244.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1400\n",
      "Train_loss: 8.59e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 1529/2000 [00:06<00:01, 244.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1500\n",
      "Train_loss: 8.734e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 1627/2000 [00:07<00:01, 234.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1600\n",
      "Train_loss: 8.417e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 1746/2000 [00:07<00:01, 229.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1700\n",
      "Train_loss: 8.924e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 1838/2000 [00:08<00:00, 224.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1800\n",
      "Train_loss: 8.568e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 1931/2000 [00:08<00:00, 227.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1900\n",
      "Train_loss: 9.058e+03\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:08<00:00, 228.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-947ddc289748>:233: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = GAN_code(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QD-p8YB_qHL"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"e.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUcKIO2G_yIr"
   },
   "outputs": [],
   "source": [
    "Data = s  \n",
    "\"\"\"\n",
    "Data: Data which has missing values represented as np.nan values \n",
    "Missing: Missing which is a mask matrix with missing values as 0 and non-missing values 1. \n",
    "return type is a dataframe with all the values filled \n",
    "\"\"\"\n",
    "#%% Packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "# Data generation\n",
    "def mask_matrix(df):\n",
    "    newdf = df.notnull().astype('int')\n",
    "    return newdf \n",
    "#Data.drop(columns= ['Time'], inplace =True)\n",
    "#Missing.drop(columns= ['Time'], inplace =True)\n",
    "Missing = mask_matrix(Data)\n",
    "Data = Data.fillna(0)\n",
    "cols = Missing.columns.values\n",
    "#cols = ['CW.FP.TONS', 'ELE.POWER','MASS.FLOW', 'OAT', 'ENTHALPY']\n",
    "Missing = Missing[cols]\n",
    "Data = np.array(Data)\n",
    "Missing = np.array(Missing)\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "Train_No =  No\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "H_Dim3 = Dim\n",
    "# Normalization (0 to 1)\n",
    "\"\"\"Min_Val = np.zeros(Dim)\n",
    "            Max_Val = np.zeros(Dim)\n",
    "            for i in range(Dim):\n",
    "                Min_Val[i] = np.min(Data[:,i])\n",
    "                Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "                Max_Val[i] = np.max(Data[:,i])\n",
    "                Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)   \"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler().fit(Data) \n",
    "\n",
    "#%% Missing introducing\n",
    "#p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "  \n",
    "#Missing = np.zeros((No,Dim))\n",
    "\n",
    "#for i in range(Dim):\n",
    "#    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "#    B = A > p_miss_vec[i]\n",
    "#    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "  \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "#Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data\n",
    "#testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing\n",
    "#testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "  \n",
    "'''\n",
    "GAIN Consists of 3 Components\n",
    "- Generator\n",
    "- Discriminator\n",
    "- Hint Mechanism\n",
    "'''   \n",
    "  \n",
    "#%% GAIN Architecture   \n",
    "  \n",
    "#%% 1. Input Placeholders\n",
    "# 1.1. Data Vector\n",
    "X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.2. Mask Vector \n",
    "M = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.3. Hint vector\n",
    "H = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.4. X with missing values\n",
    "New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "\n",
    "#%% 2. Discriminator\n",
    "D_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Hint as inputs\n",
    "D_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape = [Dim]))       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 3. Generator\n",
    "G_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "G_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "G_W3 = tf.Variable(xavier_init([H_Dim2, H_Dim3]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape = [H_Dim3]))\n",
    "\n",
    "G_W4 = tf.Variable(xavier_init([H_Dim3, Dim]))\n",
    "G_b4 = tf.Variable(tf.zeros(shape = [Dim]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_W4, G_b1, G_b2, G_b3, G_b4]\n",
    "\n",
    "#%% GAIN Function\n",
    "\n",
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "    G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3)+G_b2)\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h3, G_W4) + G_b4) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "    \n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "    #%% Structure\n",
    "# Generator\n",
    "G_sample = generator(New_X,M)\n",
    "\n",
    "# Combine with original data\n",
    "Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "# Discriminator\n",
    "D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "#%% Loss\n",
    "D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "G_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "\n",
    "D_loss = D_loss1\n",
    "G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "#%% MSE Performance metric\n",
    "#MSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n",
    "\n",
    "#%% Solver\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# Sessions\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#%% Iterations\n",
    "\n",
    "#%% Start Iterations\n",
    "for it in tqdm(range(2000)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_train_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss],\n",
    "                                                                      feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "            \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n",
    "        #print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n",
    "        print()\n",
    "\n",
    "\n",
    "New_X = generator(Data,tf.to_float(Missing))\n",
    "# Combine with original data\n",
    "Hat_New_X = Data * Missing + New_X * (1-Missing)\n",
    "final = pd.DataFrame(sess.run(Hat_New_X))\n",
    "Data =  pd.DataFrame(Data)\n",
    "col = Data.columns.values \n",
    "Data  = pd.DataFrame(scaler.transform(Data), columns = col)\n",
    "Missing =  pd.DataFrame(Missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFutqq6tfMiI"
   },
   "outputs": [],
   "source": [
    "def method_random_forest(data,label):\n",
    "        \"\"\"\n",
    "        data : There needs to be two inputs dataframe and the label name \n",
    "        label: The label name needs to be having a string format.  \n",
    "        \n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd \n",
    "        import numpy as np\n",
    "        #Split the data into training and testing sets\n",
    "        Y = data[label]\n",
    "        X = data.loc[:, data.columns != label]\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(X, Y,test_size = 0.25, random_state = 42)\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        # Instantiate model with 1000 decision trees\n",
    "        rf = RandomForestRegressor(n_estimators = 50, random_state = 42)\n",
    "        # Train the model on training data\n",
    "        rf.fit(train_features, train_labels)\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = rf.predict(test_features)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - test_labels)\n",
    "        # Print out the mean absolute error (mae)\n",
    "        #print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "        #mape = 100 * (errors / test_labels)\n",
    "        # Calculate and display accuracy\n",
    "        #accuracy = 100 - np.mean(mape)\n",
    "        return np.sqrt(((predictions - test_labels ) ** 2).mean())\n",
    "        #return print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bqneoMYcficZ",
    "outputId": "da39e656-1a94-480d-8aa2-1e520ef63bed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9139178366822065"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_random_forest(s,\"INR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cZ5uhSFpfnYO",
    "outputId": "6da60ed5-1c72-46fa-dd12-c9ae072aec7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61575, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUccgOXpgRMt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "varner_lab_work.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
